{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6a70a2-eb7d-4d0b-ac10-08eb2f3d7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, plot_roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, f1_score, plot_roc_curve, roc_auc_score, accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import thor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6f3a47-391f-460f-a3d7-1ac2ab93e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/total_train.csv')\n",
    "\n",
    "X = train['title'].str.lower()\n",
    "y = train['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22445e8f-fbf6-42d3-b591-d41812040da4",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38d9ffc-73b0-4d3e-a27e-e193e046326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = thor.tokenize(X_train, pattern='\\w\\w+')\n",
    "test_sentences =thor.tokenize(X_test, pattern = '\\w\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29b03f1-b06c-40b6-818c-479b4ac2d1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [pakistan, muslims, attack, christian, family,...\n",
       "1        [please, sign, the, petition, ezgi, needa, you...\n",
       "2        [why, evangelical, women, are, questioning, th...\n",
       "3        [episode, drama, business, proposal, dapat, ra...\n",
       "4        [pentagon, approves, 700, national, guardsmen,...\n",
       "                               ...                        \n",
       "72553    [centre, approves, continuation, of, police, m...\n",
       "72554    [justice, department, continues, to, look, at,...\n",
       "72555    [tuneskit, screen, recorder, crack, free, down...\n",
       "72556    [pawandeep, rajan, biography, winner, age, wik...\n",
       "72557    [more, than, 500, flee, to, thailand, as, rebe...\n",
       "Name: title, Length: 72558, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d8056-4a1e-4a43-9609-3e1df296f9f8",
   "metadata": {},
   "source": [
    "# Word2Vec \n",
    "\n",
    "What we will do use a pre-trained Word2Vec model to embed words as 100-dimensional vectors. The model we will use is Stanford's Glove model and we will implement via the NLP library ```gensim```. We have downloaded the pre-trained model in the ```word2vec``` folder. First we will need to convert the glove word2vec model into a gensim word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "035d2688-4400-4a88-b620-d9b9c7393e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herma\\AppData\\Local\\Temp\\ipykernel_14424\\1149461943.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400001, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file = '../word2vec/glove.6B.100d.txt'\n",
    "word2vec_output_file = '../word2vec/word2vec.txt'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa65ed9c-21a2-4acf-849d-a14705783b9e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "With the conversion complete, we can next load the model and test out some of the word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8722a6-8fbd-4bf0-afb6-51b204f829d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim.downloader as api\n",
    "#w2v = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format('../word2vec/word2vec.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc0e2c-8e5b-4f6e-9037-1f0ec8000441",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "It might be a good idea to demonstrate how a word2vec model actually works. In a nutshell, a word2vec model takes a word such as ```queen``` and maps it to a vector, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a78335-8494-46f0-a66a-2cccaeeb13a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.50045 , -0.70826 ,  0.55388 ,  0.673   ,  0.22486 ,  0.60281 ,\n",
       "       -0.26194 ,  0.73872 , -0.65383 , -0.21606 , -0.33806 ,  0.24498 ,\n",
       "       -0.51497 ,  0.8568  , -0.37199 , -0.58824 ,  0.30637 , -0.30668 ,\n",
       "       -0.2187  ,  0.78369 , -0.61944 , -0.54925 ,  0.43067 , -0.027348,\n",
       "        0.97574 ,  0.46169 ,  0.11486 , -0.99842 ,  1.0661  , -0.20819 ,\n",
       "        0.53158 ,  0.40922 ,  1.0406  ,  0.24943 ,  0.18709 ,  0.41528 ,\n",
       "       -0.95408 ,  0.36822 , -0.37948 , -0.6802  , -0.14578 , -0.20113 ,\n",
       "        0.17113 , -0.55705 ,  0.7191  ,  0.070014, -0.23637 ,  0.49534 ,\n",
       "        1.1576  , -0.05078 ,  0.25731 , -0.091052,  1.2663  ,  1.1047  ,\n",
       "       -0.51584 , -2.0033  , -0.64821 ,  0.16417 ,  0.32935 ,  0.048484,\n",
       "        0.18997 ,  0.66116 ,  0.080882,  0.3364  ,  0.22758 ,  0.1462  ,\n",
       "       -0.51005 ,  0.63777 ,  0.47299 , -0.3282  ,  0.083899, -0.78547 ,\n",
       "        0.099148,  0.039176,  0.27893 ,  0.11747 ,  0.57862 ,  0.043639,\n",
       "       -0.15965 , -0.35304 , -0.048965, -0.32461 ,  1.4981  ,  0.58138 ,\n",
       "       -1.132   , -0.60673 , -0.37505 , -1.1813  ,  0.80117 , -0.50014 ,\n",
       "       -0.16574 , -0.70584 ,  0.43012 ,  0.51051 , -0.8033  , -0.66572 ,\n",
       "       -0.63717 , -0.36032 ,  0.13347 , -0.56075 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['queen']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a390a6-042b-4ec6-8282-3bba3d5d8cef",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The model converts ```queen``` to a vector by training a neural network. Essentially what the model does is:\n",
    "- look at the sentences with the word ```queen```.\n",
    "- For each sentence, mark down all the words that appear near it, e.g. ```prince```, ```mother```, ```highness```, etc. These are called **context words**.\n",
    "- It then looks for other words with the similar context words such as, eg. ```king``` and adjusts the vector entries for ```queen``` closer to that of ```king```.\n",
    "\n",
    "The pre-trained model we are using is Stanford's Glove model, which has 6 billion words trained on Wikipedia pages.\n",
    "\n",
    "Because word2vec transforms words to vectors, we can now add and subtract words like so: if we take ```king``` minus ```man``` plus ```woman```, the resulting vector is closest to ```queen``` (with similarity score of 0.77)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "489a00de-830a-4374-a476-f218d89e4575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03120e98-fcb6-4e4d-b4fc-12f6180b4e36",
   "metadata": {},
   "source": [
    "The similarity score between two vectors is measured via the cosine of the angle formed by the two vectors. Intuitively:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\cos(0)=1 \\qquad \\qquad \\cos\\left(\\frac{\\pi}{2}\\right)=0$$\n",
    "\n",
    "<br>\n",
    "\n",
    "So two vectors which are similar will have small angle $\\theta$ and consequently will have a $\\cos(\\theta)$ near 1. On the other hand two vectors which are \"independent\" would be nearly orthogonal and hence have nearly $\\cos(\\theta)$ near 0. \n",
    "\n",
    "To compute the cosine of the angle, we can use the Law of Cosines to derive the following formula:\n",
    "$$\\textbf{v}\\cdot \\textbf{u} = |v||u|\\cos(\\theta)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\cos(\\theta)=\\frac{\\textbf{v}\\cdot \\textbf{u}}{|v||u|}$$\n",
    "\n",
    "So the number 0.77 (in the similarity score outputted above) is actually the cosine of the angle between the vector $\\textbf{v}$ = ```king``` - ```man``` + ```woman``` and the vectur $\\textbf{u}$ = ```queen```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2644bd2b-7238-495f-be9c-56404bae3f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7834415"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = w2v['king'] - w2v['man'] + w2v['woman']\n",
    "\n",
    "np.dot(v, w2v['queen'])/(np.linalg.norm(v)*np.linalg.norm(w2v['queen']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf03dc-ecf2-4c95-b39a-136a5b6e1e2d",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "\n",
    "Word2vec is *infamous* for implicitly learning social bias because of how society uses language. One (in)famous example demonstrating gender and sexual bias is the following: ```doctor```-```man```+```woman```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e00490ab-e1c7-4e3f-90c7-7649ce78471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nurse', 0.7735227942466736)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive = ['doctor', 'woman'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebda3d-79e7-4cd7-a075-1db38dfbfd7a",
   "metadata": {},
   "source": [
    "Notice that word2vec has implicitly learned gender bias since it has associated the word ```woman``` with the occupation of being a ```nurse```. A bit more of an insidious take: word2vec doesn't recognize the existence of woman doctors.\n",
    "\n",
    "Since we are using Word2Vec for the purposes of extrapolating word \"context\", it is good to be aware of this rather unfortunate property of Word2Vec. \n",
    "\n",
    "Anyways, this is how we will be using Word2Vec:\n",
    "\n",
    "1. For each document: embed each of the words as a 100-dimensional vector. This gives us an array of 100-D vectors a.k.a. a **tensor**.\n",
    "2. Concatenate this array into one single very long vector (3000-dimensional to be exact). Now we have transformed our document into a vector but unlike a simple frequency count, the values of the vector correspond to word meaning and word context. We have also preserved word ordering as well.\n",
    "3. Collect these vectors as rows of a matrix. Now we have a matrix representation of our corpus which we can train a model on.\n",
    "\n",
    "The hope is that by using word embeddings, we have found a way to encode some semblence of \"context\" and \"meaning\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22eecdd3-493f-4ad6-a0f9-bb27d82ec1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn each word into a 100-dimensional vector. Words which are not in our vocabulary will have to be encoded as all 0's\n",
    "def word_to_vector(word):\n",
    "    try:\n",
    "        return w2v[word]\n",
    "    except:\n",
    "        return np.zeros(100)\n",
    "    \n",
    "# turn a document into a chain of 2500-dimensional vector, with each word occupying a slice of size 100.   \n",
    "def sent_to_vector(sent):\n",
    "    if len(sent) == 0:\n",
    "        return np.zeros(3000)\n",
    "    \n",
    "    elif len(sent) >= 30:\n",
    "        return np.concatenate([word_to_vector(word) for word in sent[0:30]])\n",
    "    \n",
    "    else:\n",
    "        n = 30-len(sent)\n",
    "        return np.concatenate([word_to_vector(word) for word in sent]+[[0]*100]*n)\n",
    "\n",
    "# turn the corpus into a matrix\n",
    "def doc_to_vector(ser):\n",
    "    return [sent_to_vector(sent) for sent in ser]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1e22d7e-fb7c-41c6-9f9b-0c01afafee27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xvec_train = doc_to_vector(train_sentences)\n",
    "\n",
    "Xvec_test = doc_to_vector(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e85651-d997-4a3f-ade3-9e466398edfe",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "With our corpus properly formatted, let's try to train a model on this and see what happens. With such a high number of training examples and only 3000 input dimensions, Logistic Regression seems like a good first choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36015561-5836-467a-a30d-5716a346d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logvec = LogisticRegression(max_iter=20_000, C=1)\n",
    "\n",
    "#logvec.fit(Xvec_train, y_train)\n",
    "\n",
    "#joblib.dump(logvec, '../models/log2vec.pkl')\n",
    "\n",
    "logvec = joblib.load('../models/log2vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e83528-1ace-4b0a-b3cd-203911466b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9136690647482014, 0.8958074919374845)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logvec.score(Xvec_train, y_train), logvec.score(Xvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92bd7624-5c96-43ad-b739-780348dac37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Accuracy:  0.8958074919374845\n",
      " \n",
      "Recall:  0.3721841332027424\n",
      " \n",
      "Precision:  0.6563039723661486\n",
      " \n",
      "F1:  0.475\n",
      " \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thor.verbose_eval(logvec, Xvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1d1e794-3dbb-4bd7-b52c-24a9a5cee0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler()\n",
    "\n",
    "Xover_train, y_over_train = ros.fit_resample(Xvec_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9ef5500-473a-4230-82a1-e112a8c32127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logvec_over = LogisticRegression(max_iter=20_000, C=1)\n",
    "\n",
    "#logvec_over.fit(Xover_train, y_over_train)\n",
    "\n",
    "#joblib.dump(logvec_over, '../models/log2vec_over.pkl')\n",
    "\n",
    "logvec_over = joblib.load('../models/log2vec_over.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ee2fd04-f1d4-4cd8-9ebc-69e3280b74a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8253109020895145, 0.8004217315802531)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logvec_over.score(Xover_train, y_over_train), logvec_over.score(Xvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17e1ed23-43fa-4c7f-9c52-59925bd25fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Accuracy:  0.8004217315802531\n",
      " \n",
      "Recall:  0.7247796278158668\n",
      " \n",
      "Precision:  0.3578336557059961\n",
      " \n",
      "F1:  0.4791194561346714\n",
      " \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thor.verbose_eval(logvec_over, Xvec_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777801b4-5a9b-4e6d-9cdb-9529020a06cc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Rather interestingly, this model is significantly worse than our previous models using only count frequencies as features. This is rather alarming since we have (at least theoretically) encoded more information for the model to use and yet the model isn't able to use it.\n",
    "\n",
    "This leads us to suspect that we need a more sophisticated model. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3889a-0c29-427a-b48d-92133ea77228",
   "metadata": {},
   "source": [
    "# OnionNet: a Simple 1-Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5e5a87a-f4b8-45b2-9932-b69914f00ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad81114a-d178-4238-99b3-48736db5e7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2e52027-ec55-4490-a4ed-ea8807a924b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array_train = np.array(Xvec_train)\n",
    "y_array_train = np.array(y_train)\n",
    "\n",
    "X_array_test = np.array(Xvec_test)\n",
    "y_array_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13a7e1d7-f96c-41d1-ab23-14cec6b9fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onionnet = keras.Sequential([\n",
    "                keras.Input(shape=(3000,)),\n",
    "                keras.layers.Dense(600, activation = 'relu', activity_regularizer = keras.regularizers.l2(0.001)),\n",
    "                keras.layers.Dense(1, activation = 'sigmoid')\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbeeb1bd-836a-4206-baac-d40b38d62201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 600)               1800600   \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 601       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,801,201\n",
      "Trainable params: 1,801,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "onionnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "177fc6cb-07d0-4da3-a892-9f22fab3344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onionnet.compile(\n",
    "        loss = 'binary_crossentropy',\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics = ['accuracy']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "257342f4-cf59-4e57-9969-d6617965af6e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 - 1s - loss: 0.2214 - accuracy: 0.9296 - val_loss: 0.4271 - val_accuracy: 0.8792 - 1s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "500/500 - 1s - loss: 0.2223 - accuracy: 0.9288 - val_loss: 0.4269 - val_accuracy: 0.8802 - 895ms/epoch - 2ms/step\n",
      "Epoch 3/20\n",
      "500/500 - 1s - loss: 0.2211 - accuracy: 0.9302 - val_loss: 0.4348 - val_accuracy: 0.8767 - 848ms/epoch - 2ms/step\n",
      "Epoch 4/20\n",
      "500/500 - 1s - loss: 0.2187 - accuracy: 0.9302 - val_loss: 0.4199 - val_accuracy: 0.8823 - 810ms/epoch - 2ms/step\n",
      "Epoch 5/20\n",
      "500/500 - 1s - loss: 0.2176 - accuracy: 0.9308 - val_loss: 0.4303 - val_accuracy: 0.8823 - 792ms/epoch - 2ms/step\n",
      "Epoch 6/20\n",
      "500/500 - 1s - loss: 0.2161 - accuracy: 0.9307 - val_loss: 0.4355 - val_accuracy: 0.8783 - 792ms/epoch - 2ms/step\n",
      "Epoch 7/20\n",
      "500/500 - 1s - loss: 0.2163 - accuracy: 0.9314 - val_loss: 0.4405 - val_accuracy: 0.8783 - 848ms/epoch - 2ms/step\n",
      "Epoch 8/20\n",
      "500/500 - 1s - loss: 0.2136 - accuracy: 0.9317 - val_loss: 0.4423 - val_accuracy: 0.8785 - 823ms/epoch - 2ms/step\n",
      "Epoch 9/20\n",
      "500/500 - 1s - loss: 0.2149 - accuracy: 0.9315 - val_loss: 0.4619 - val_accuracy: 0.8785 - 812ms/epoch - 2ms/step\n",
      "Epoch 10/20\n",
      "500/500 - 1s - loss: 0.2143 - accuracy: 0.9311 - val_loss: 0.4419 - val_accuracy: 0.8750 - 790ms/epoch - 2ms/step\n",
      "Epoch 11/20\n",
      "500/500 - 1s - loss: 0.2133 - accuracy: 0.9316 - val_loss: 0.4533 - val_accuracy: 0.8805 - 816ms/epoch - 2ms/step\n",
      "Epoch 12/20\n",
      "500/500 - 1s - loss: 0.2133 - accuracy: 0.9317 - val_loss: 0.4447 - val_accuracy: 0.8783 - 819ms/epoch - 2ms/step\n",
      "Epoch 13/20\n",
      "500/500 - 1s - loss: 0.2124 - accuracy: 0.9316 - val_loss: 0.4502 - val_accuracy: 0.8740 - 840ms/epoch - 2ms/step\n",
      "Epoch 14/20\n",
      "500/500 - 1s - loss: 0.2100 - accuracy: 0.9316 - val_loss: 0.4487 - val_accuracy: 0.8727 - 897ms/epoch - 2ms/step\n",
      "Epoch 15/20\n",
      "500/500 - 1s - loss: 0.2097 - accuracy: 0.9322 - val_loss: 0.4653 - val_accuracy: 0.8777 - 926ms/epoch - 2ms/step\n",
      "Epoch 16/20\n",
      "500/500 - 1s - loss: 0.2100 - accuracy: 0.9324 - val_loss: 0.4605 - val_accuracy: 0.8752 - 826ms/epoch - 2ms/step\n",
      "Epoch 17/20\n",
      "500/500 - 1s - loss: 0.2114 - accuracy: 0.9320 - val_loss: 0.4637 - val_accuracy: 0.8773 - 793ms/epoch - 2ms/step\n",
      "Epoch 18/20\n",
      "500/500 - 1s - loss: 0.2087 - accuracy: 0.9318 - val_loss: 0.4695 - val_accuracy: 0.8755 - 804ms/epoch - 2ms/step\n",
      "Epoch 19/20\n",
      "500/500 - 1s - loss: 0.2095 - accuracy: 0.9325 - val_loss: 0.4694 - val_accuracy: 0.8748 - 841ms/epoch - 2ms/step\n",
      "Epoch 20/20\n",
      "500/500 - 1s - loss: 0.2086 - accuracy: 0.9326 - val_loss: 0.4617 - val_accuracy: 0.8740 - 886ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c784f96ee0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onionnet.fit(X_array_train, y_array_train, epochs = 20, verbose=2, validation_data = (X_array_test, y_array_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "024d261f-e873-432b-ab92-3087c45c0367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4213024377822876, 0.8822500109672546]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onionnet.evaluate(X_array_test, y_array_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed75e02-8423-409b-81e2-3d7a603a2f26",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Well, we've thrown everything and the kitchen sink at this data set and we still can't seem to break improve upon the base Logistic Regression model using only count frequencies. This seems to indicate that data itself might just inherently be misbehaving and a bit too dirty. One approach is to find more clean data or to generate a hand-labelled set of simpler training examples.\n",
    "\n",
    "An alternative approach is to reduce the scope of the problem and try to solve a simpler problem first. For example, we could try instead to classify news articles based on political subreddits which emphasize both a distinct tone as well as distinct keywords.\n",
    "\n",
    "Another major issue might be the fact that we inherently set-up the problem as one of imbalanced classes. It may be the case that results would be much better if we had set-up the problem where the classes were more balanced. We can check quickly if such a set-up would in fact help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848160ac-0592-4363-bb94-8767f3688511",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sample_train = train[ train['class'] == 0].iloc[0:10_000]\n",
    "onion_sample_train = train[ train['class'] == 1].iloc[0:10_000]\n",
    "\n",
    "\n",
    "balanced_train = pd.concat([news_sample_train, onion_sample_train]).reset_index(drop=True)\n",
    "\n",
    "X = balanced_train['title']\n",
    "y = balanced_train['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_sentences = thor.tokenize(X_train, pattern='\\w\\w+')\n",
    "test_sentences =thor.tokenize(X_test, pattern = '\\w\\w+')\n",
    "\n",
    "Xvec_train = doc_to_vector(train_sentences)\n",
    "Xvec_test = doc_to_vector(test_sentences)\n",
    "\n",
    "X_array_train = np.array(Xvec_train)\n",
    "y_array_train = np.array(y_train)\n",
    "\n",
    "X_array_test = np.array(Xvec_test)\n",
    "y_array_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ac05451-b52f-4dec-8989-f16b45b4247d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.5\n",
       "0    0.5\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e46ffd57-6743-4dd2-a562-1f08a92444e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 - 2s - loss: 0.2291 - accuracy: 0.9304 - val_loss: 0.3919 - val_accuracy: 0.8865 - 2s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 1s - loss: 0.2241 - accuracy: 0.9316 - val_loss: 0.4000 - val_accuracy: 0.8873 - 982ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 1s - loss: 0.2236 - accuracy: 0.9312 - val_loss: 0.4146 - val_accuracy: 0.8875 - 925ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 1s - loss: 0.2237 - accuracy: 0.9308 - val_loss: 0.4034 - val_accuracy: 0.8827 - 915ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 1s - loss: 0.2187 - accuracy: 0.9316 - val_loss: 0.3926 - val_accuracy: 0.8852 - 925ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 1s - loss: 0.2230 - accuracy: 0.9314 - val_loss: 0.3876 - val_accuracy: 0.8800 - 922ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 1s - loss: 0.2185 - accuracy: 0.9311 - val_loss: 0.3935 - val_accuracy: 0.8840 - 947ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 1s - loss: 0.2199 - accuracy: 0.9317 - val_loss: 0.4099 - val_accuracy: 0.8808 - 924ms/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 1s - loss: 0.2225 - accuracy: 0.9305 - val_loss: 0.4111 - val_accuracy: 0.8848 - 932ms/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 1s - loss: 0.2237 - accuracy: 0.9313 - val_loss: 0.4267 - val_accuracy: 0.8823 - 954ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c84b030af0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onionnet2 = keras.Sequential([\n",
    "                keras.Input(shape=(3000,)),\n",
    "                keras.layers.Dense(600, activation = 'relu', activity_regularizer = keras.regularizers.l2(0.0001)),\n",
    "                keras.layers.Dense(1, activation = 'sigmoid')\n",
    "            ])\n",
    "\n",
    "onionnet.compile(\n",
    "        loss = 'binary_crossentropy',\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics = ['accuracy']\n",
    "        )\n",
    "\n",
    "onionnet.fit(X_array_train, y_array_train, epochs = 10, verbose=2, validation_data = (X_array_test, y_array_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191bb639-170c-4820-b4d7-59b8dee854d8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Balancing the has pushed it above the baseline accuracy but it seems there is something inherently stopping it from being able to improve.\n",
    "\n",
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "So it seems that this word2vec encoding of our documents does not work as well as simple frequency encodings from our previous models. This leads us to suggest that there is something inherent in the data (or perhaps in the encoding method) which is preventing a decent model from being trained.\n",
    "\n",
    "We notice that our neural network seems to reduce training loss at the expense of increasing cross-validation loss. We are currently not sure why this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5242b2-3535-4af2-acb2-dcc257bb44e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Further Investigations\n",
    "\n",
    "We end our project with a proposal for a possible avenue of future investigation. Recall that the whole premise of using word2vec was to try and preserve word ordering and context clues. Since we are interested in extracting information from word ordering, it might be possible to achieve some better results using a Convolutional Neural Network. Essentially, the idea is to achieve a sort of 3-gram grouping by convolving sequential sub-vectors of length 300 (moving at steps of 100 per word). In the same way that images can be paced-together by convolving sub-matrices of the pixel matrix using a CNN, it might be possible to achieve something analogous to this for a sentence structure.\n",
    "\n",
    "Another thing worth trying is to run the neural network on the corpus using frequence encoding. Since a basic logistic regression model was abel to do so well using this method, we suspect a neural network may do slightly better and scale better to an even larger data set. At the moment, our hardware limitations prevent us from representing the frequency encoded corpus as a dense matrix, which we would need to do in order to input into the ```keras.Sequential()``` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc541a11-9683-4c87-b55b-f2492a0ccdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc88d1-289b-4a59-baac-0ba9eb34ed38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
