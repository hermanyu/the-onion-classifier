{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75690f6-d3d6-4375-a410-7cc82f16391e",
   "metadata": {},
   "source": [
    "# Classification of Reddit Posts via Natural Language Processing\n",
    "\n",
    "The objective of this project is to correctly classify posts between different subreddits based purely on the text data. For the purposes of this project, we have picked 3 subreddit pairings which the ascending in difficulty based on the content and nature of the posts:\n",
    "1. ```r/datascience``` vs ```r/machinelearning```.\n",
    "2. ```r/news``` vs ```r/theonion```.\n",
    "3. ```r/theonion``` vs ```r/nottheonion```.\n",
    "\n",
    "The idea is to start by prototyping on an easy problem and then progressively ratchet up the (perceived) difficulty to motivate the development of an increasingly complex and robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a30eb0-8639-448e-b496-cad6889feb21",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Data Collection and Scraping\n",
    "\n",
    "We collect posts off reddit using the ```pushshift.io``` API. We package the data-scaping steps and oft-used parameters into helper functions in the custom ```bifrost``` library (this is a reference to the Rainbow Bridge in Norse mythology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464b4944-0294-4d63-b616-34c4bb4cc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import bifrost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c590ef-6504-4a7e-bc90-632da3eb097a",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "Our first step will be to scrape necessary data from the subreddits in question. The first question me must answer is:\n",
    "     \n",
    "<h2 align=\"center\">How much data should we scrape?</h2>\n",
    "     \n",
    "One might think the answer is simple: \"As much as we can get!\" but this is not a satisfactory answer. We could easily scrape 1,000,000 reddit posts from each of the 5 subreddits and this would give us an abundant wealth of data. However, vectorization and training on 2,000,000 text-based examples would be woefully slow and may even be impossible given hardware limitations (imagine trying to store a $2,000,000 \\times 100,000$ matrix in memory; it would probably eat up every last byte of RAM we have and still not be enough!).\n",
    "\n",
    "We must let level-heads prevail and choose a more reasonable number. Let us decide (a bit arbitrarily) on 10,000 posts to train on for each subreddit, with an additional 5,000 posts held-out as a final test set to give our model a final grade. If it turns out this number is too small, we can always go back and scrape more data.\n",
    "\n",
    "We proceed by first scraping the 10,000 training posts from each subreddit and logging the UTC creation times. Then after building our models, we will scrape the additional 5,000 posts with UTC creation times before the last training example (this separation of data scraping is a safe-guard against accidentally training on the test set).\n",
    "\n",
    "We provide the following code as a markdown cell to avoid having to wait for the web-scraping process each time we reboot this notebook, but the reader is free to try running it themselves in a code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66288b31-43b6-430e-8091-6f5a37e8197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_params = {'subreddit':'datascience',\n",
    "             'fields':('title', 'selftext','created_utc'),\n",
    "             'size':100}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58845a-5a5c-4ecb-be45-cc26e1bc64ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "\n",
    "datascience = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( # scrapes subreddit for content, 0.8s time-delay built-in to avoid accidental DDoS\n",
    "                                        content = 'submission',\n",
    "                                        params=ds_params, \n",
    "                                        iters = 100,  # 1 iteration collects 100 posts.\n",
    "                                        verbose = True  # demonstrate process with console printout, will be turned off in future calls\n",
    "                                )\n",
    "                          )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7c6eb-bf95-4ced-94dd-111d16dc0123",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "And just like that, ```bifrost``` has scraped 10,000 reddit posts from ```r/datascience```.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8daaf0d-df26-4545-9b72-566fd15df082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously scraped results \n",
    "# this avoids having to re-scrape each time we reboot the notebook :)\n",
    "datascience = pd.read_csv('../data/datascience10k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001d6aa-814c-46ac-8433-202e0f5bf3e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "# save our data to a csv so we don't have to re-scrape every single time\n",
    "datascience.to_csv('../data/datascience10k.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b641aef9-d351-4ad0-9694-6b48e4fa3b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "datascience.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e206381-2f7f-454e-b5da-45a1461d0f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'created_utc', 'selftext', 'title'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26db551b-19b9-4669-a6e0-97163b539d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "datascience.drop(columns='Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a86f6c-838b-475a-b106-d07a7720c857",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "Let's check for null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a55a18-4910-4e24-bc02-e1003190f6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_utc       0\n",
       "selftext       1726\n",
       "title             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d098b-f175-48cc-b545-28cc577594ff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We have approximately 1700 empty bodies. Based on the author's reddit experience, these are most likely either:\n",
    "1. Questions: the question is in the title so body is empty.\n",
    "2. Content Sharing: the actual content is not text-based, e.g. an embedded youtube video for NLP.\n",
    "3. Removed: the post contents were removed by the moderators.\n",
    "\n",
    "Let's take a quick look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3681a139-06b9-409f-81fb-627a0a31852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1648041965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are your favorite reference books?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1648032739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data scientist returning to work from maternit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1648029230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Logical process order of a sql query and why k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1648024571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hypothesis Testing &amp;amp; Anova Model Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1648018824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Could you give me idea for entry level project...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc selftext                                              title\n",
       "20   1648041965      NaN            What are your favorite reference books?\n",
       "26   1648032739      NaN  Data scientist returning to work from maternit...\n",
       "28   1648029230      NaN  Logical process order of a sql query and why k...\n",
       "30   1648024571      NaN        Hypothesis Testing &amp; Anova Model Topics\n",
       "34   1648018824      NaN  Could you give me idea for entry level project..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience[ datascience['selftext'].isnull() ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97770de-6768-4940-8ac0-f24383f9e13b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's also now scrape the data for the other 4 subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbf38968-d4c0-476f-b078-f3e15bbaf7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_params = {'subreddit':'machinelearning','fields':('title', 'selftext','created_utc'),'size':100}\n",
    "\n",
    "news_params = {'subreddit':'news','fields':('title', 'selftext','created_utc'),'size':100}\n",
    "\n",
    "onion_params = {'subreddit':'theonion','fields':('title', 'selftext','created_utc'),'size':100}\n",
    "\n",
    "notonion_params = {'subreddit':'nottheonion','fields':('title', 'selftext','created_utc'),'size':100}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ce0f1-8c59-45bf-8085-a05854aacacb",
   "metadata": {},
   "source": [
    "```python\n",
    "ml = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit(\n",
    "                                        content = 'submission',\n",
    "                                        params= ml_params, \n",
    "                                        iters = 100,  \n",
    "                                        verbose = False  \n",
    "                                )\n",
    "                          )\n",
    "\n",
    "ml.to_csv('../data/machinelearning10k.csv')\n",
    "```\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54cebc-cd68-408b-b126-167e6807cd2f",
   "metadata": {},
   "source": [
    "```python\n",
    "news = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( \n",
    "                                        content = 'submission',\n",
    "                                        params= news_params, \n",
    "                                        iters = 100,  \n",
    "                                        verbose = False  \n",
    "                                )\n",
    "                          )\n",
    "\n",
    "news.to_csv('../data/news10k.csv')\n",
    "```\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152fa8a-32a3-492f-a2dc-305d55014911",
   "metadata": {},
   "source": [
    "```python\n",
    "onion = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( \n",
    "                                        content = 'submission',\n",
    "                                        params= onion_params, \n",
    "                                        iters = 100, \n",
    "                                        verbose = False  \n",
    "                          )\n",
    "\n",
    "onion.to_csv('../data/onion10k.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9acf2-5f2e-439f-9689-fdaef36ef04e",
   "metadata": {},
   "source": [
    "```python\n",
    "notonion = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( \n",
    "                                        content = 'submission',\n",
    "                                        params= notonion_params, \n",
    "                                        iters = 100, \n",
    "                                        verbose = False  \n",
    "                                )\n",
    "                          )\n",
    "\n",
    "notonion.to_csv('../data/notonion10k.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46868e2d-59ef-40aa-879b-dfb648ef6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = pd.read_csv('../data/machinelearning10k.csv')\n",
    "news = pd.read_csv('../data/news10k.csv')\n",
    "onion = pd.read_csv('../data/onion10k.csv')\n",
    "notonion = pd.read_csv('../data/notonion10k.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54775b-65be-4668-8086-5505679f6f27",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "--- \n",
    "# 100,000 Post Titles\n",
    "\n",
    "For future usage and general data hoarding purposes, we'll scrape the post titles of 100,000 reddits posts in each of the 5 subreddits mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ab7b44-0108-4ba1-9479-87c6b2e5ada2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_epoch = datascience['created_utc'][9999]\n",
    "ml_epoch = ml['created_utc'][9999]\n",
    "news_epoch = news['created_utc'][9999]\n",
    "onion_epoch = onion['created_utc'][9984]\n",
    "notonion_epoch = notonion['created_utc'][9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "403831a3-fd65-43c9-8dbd-a1c9b2cb5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_titles_params =  {'subreddit':'datascience','fields':('title','created_utc'),'size':100, 'before':ds_epoch}\n",
    "\n",
    "ml_titles_params = {'subreddit':'machinelearning','fields':('title','created_utc'),'size':100, 'before':ml_epoch}\n",
    "\n",
    "news_titles_params = {'subreddit':'news','fields':('title','created_utc'),'size':100, 'before':news_epoch}\n",
    "\n",
    "onion_titles_params = {'subreddit':'theonion','fields':('title','created_utc'),'size':100, 'before':onion_epoch}\n",
    "\n",
    "notonion_titles_params = {'subreddit':'nottheonion','fields':('title', 'created_utc'),'size':100, 'before':notonion_epoch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374fae6-5042-4fec-90a1-b61991086b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( \n",
    "                                        content = 'submission',\n",
    "                                        params= ml_titles_params, \n",
    "                                        iters = 4000,  \n",
    "                                        verbose = False,\n",
    "                                        delay=True\n",
    "                                )\n",
    "                          )\n",
    "\n",
    "ml.to_csv('../data/machinelearningtitles100k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3baf815-3273-4c7e-a85b-a2fb44a64674",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit(\n",
    "                                        content = 'submission',\n",
    "                                        params= news_titles_params, \n",
    "                                        iters = 4000, \n",
    "                                        verbose = False,\n",
    "                                        delay=True\n",
    "                                )\n",
    "                          )\n",
    "\n",
    "news.to_csv('../data/newstitles100k.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100588c-add9-4dfc-9789-38de791c0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onion = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit(\n",
    "                                        content = 'submission',\n",
    "                                        params= onion_titles_params, \n",
    "                                        iters = 4000,  \n",
    "                                        verbose = False,\n",
    "                                        delay=True\n",
    "                                )\n",
    "                          )\n",
    "\n",
    "onion.to_csv('../data/oniontitles100k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac13868-8381-472f-a8a8-33587846fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "notonion = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit(\n",
    "                                        content = 'submission',\n",
    "                                        params= notonion_titles_params, \n",
    "                                        iters = 4000,  \n",
    "                                        verbose = False,\n",
    "                                        delay=True\n",
    "                                )\n",
    "                          )\n",
    "\n",
    "notonion.to_csv('../data/notoniontitles100k.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
