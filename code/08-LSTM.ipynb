{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0142c0-468d-4115-a6cf-94c7d95cec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, plot_roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, f1_score, plot_roc_curve, roc_auc_score, accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import thor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06cdcb0-c177-444d-9265-9478279b3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/total_train.csv')\n",
    "\n",
    "X = train['title'].str.lower()\n",
    "y = train['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a22e108-9938-426b-a41a-0cad2ebc8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "train_sentences = thor.tokenize(X_train, pattern='\\w\\w+')\n",
    "test_sentences =thor.tokenize(X_test, pattern = '\\w\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf96e62-299f-4f2b-91e9-86a6e3e59914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [common, signs, of, cervical, cancer]\n",
       "1        [cubana, chief, priest, hires, rain, makers, t...\n",
       "2        [geomagnetic, storm, may, have, effectively, d...\n",
       "3        [masten, wanjala, mob, beats, kenyan, child, s...\n",
       "4        [astro, founding, member, of, the, reggae, gro...\n",
       "                               ...                        \n",
       "72291    [urgent, call, for, sperm, donors, as, birming...\n",
       "72292    [tiktok, suspends, livestreaming, and, new, up...\n",
       "72293    [seal, lying, in, sunbeam, could, be, depresse...\n",
       "72294    [lumion, 11, crack, amp, serial, key, full, ve...\n",
       "72295    [the, appearance, of, the, monuments, will, be...\n",
       "Name: title, Length: 72296, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c763bf-d6dc-4e21-8e8c-b88a9da3918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herma\\AppData\\Local\\Temp\\ipykernel_8072\\1605258110.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "# import GloVe model\n",
    "glove_input_file = '../word2vec/glove.6B.100d.txt'\n",
    "word2vec_output_file = '../word2vec/word2vec.txt'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format('../word2vec/word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a71fb2c-10cf-4998-8fe9-6c46ddb8f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn each word into a 100-dimensional vector. Words which are not in our vocabulary will have to be encoded as all 0's\n",
    "def word_to_vector(word):\n",
    "    try:\n",
    "        return np.array(w2v[word])\n",
    "    except:\n",
    "        return np.zeros(100)\n",
    "    \n",
    "# turn a document an array of 30 vectors\n",
    "def sent_to_tensor(sent):\n",
    "    n = len(sent)\n",
    "    # if the sentence is less than 30 words, we'll need to add some padding\n",
    "    # note: we pad the front; this is because of how an RNN flows and what we are trying to extract\n",
    "    if n < 30:\n",
    "        return np.array([np.zeros(100) for i in range(30-n)]+[word_to_vector(word) for word in sent])\n",
    "    else:\n",
    "        return np.array([word_to_vector(sent[i]) for i in range(30)])\n",
    "\n",
    "# turn the corpus into a 3-tensor\n",
    "def corpus_to_vector(ser):\n",
    "    return np.array([sent_to_tensor(sent) for sent in ser])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4437762-586d-4d9b-b8f2-e883efacd3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.1529    , -0.24279   ,  0.89837003, ..., -0.59100002,\n",
       "         1.00390005,  0.20664001],\n",
       "       [ 0.092774  ,  0.98281997, -0.25476   , ..., -0.54462999,\n",
       "        -0.34422001, -0.90757   ],\n",
       "       [ 0.30875999,  0.57172   , -0.76573002, ..., -0.49552   ,\n",
       "         0.36862999, -0.43560001]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_to_tensor(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2bc8e19f-28d9-4a67-8f36-ca036de10910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to vector embeddings\n",
    "Xvec_train = corpus_to_vector(train_sentences)\n",
    "\n",
    "Xvec_test = corpus_to_vector(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c536b2a-8436-45d4-9844-4507778e1f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72296, 30, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvec_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca18e7d2-d0f3-493a-bcd2-24a52b44c554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.1529    , -0.24279   ,  0.89837003, ..., -0.59100002,\n",
       "         1.00390005,  0.20664001],\n",
       "       [ 0.092774  ,  0.98281997, -0.25476   , ..., -0.54462999,\n",
       "        -0.34422001, -0.90757   ],\n",
       "       [ 0.30875999,  0.57172   , -0.76573002, ..., -0.49552   ,\n",
       "         0.36862999, -0.43560001]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvec_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c51a8c8-4061-4f04-9b04-9fc1cb84b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert embeddings to PyTorch tensors\n",
    "Xten_train = torch.tensor(Xvec_train, dtype=torch.float32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fca74c51-aeb0-4418-9b12-f8ac929275bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OnionNet Model with LSTM layer\n",
    "class OnionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OnionNet, self).__init__()\n",
    "        # The first hidden layer is an LSTM layer. It takes as input shapes of (batch_size, sequence length, embedding)\n",
    "        # The output will be of shape (sequence length, hidden units)\n",
    "        self.lstm = nn.LSTM(input_size = 100, hidden_size = 64, batch_first=True, device='cuda')\n",
    "        # add one more hidden layer (just for experimentation)\n",
    "        self.hidden = nn.Linear(in_features = 64, out_features = 32, device='cuda')\n",
    "        # output layer. Since we're using BCELoss, we'll need to run this through a sigmoid\n",
    "        self.output = nn.Linear(in_features = 32, out_features = 1, device='cuda')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # because lstm_out returns an output for each of the 30 tokens\n",
    "        # we'll need to manually extract the last output value (as it \"summarizes\" the entire sentenc)\n",
    "        z = lstm_out[-1]\n",
    "        z = self.tanh(self.hidden(z))\n",
    "        z = self.output(z)\n",
    "        z = self.sigmoid(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3575091-2846-45bf-994b-35470bc998d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 1]), torch.Size([1, 1]), torch.Size([1, 1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play around with the LSTM layer to see the input / output sizes\n",
    "lstm_out, (h_n, c_n) = nn.LSTM(input_size=100, hidden_size=1, batch_first=True)(torch.tensor(Xvec_train[1], dtype=torch.float32))\n",
    "\n",
    "\n",
    "lstm_out.shape, h_n.shape, c_n.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb66885-caf4-4d33-af31-8e37a7eebc81",
   "metadata": {},
   "source": [
    "- Notice that ```lstm_out``` is an array of size 30. This is because we had 30 tokens per sentence and the LSTM returns an output for each token.\n",
    "- Of course, output we actually care about is the one at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd7fa23f-6c72-4906-ae36-7c74868a6702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1970], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb43229b-93e8-405a-8151-959538289483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recast labels as pytorch tensor\n",
    "y2_train = torch.tensor(y_train, dtype=torch.float32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0748a87-0f9b-4c7f-8869-cfae9dc842eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape for training purposes\n",
    "y2_train = y2_train.view(y2_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f7b9f83-010f-41ce-9815-e89f37b09afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e16d289-48be-41f0-940f-ae3f3de911bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model, loss, and optimizer\n",
    "onionnet = OnionNet()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(onionnet.parameters(), lr = 0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84775106-979c-48c8-be7e-92ab49ddd9d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Epoch 1 =======================\n",
      " \n",
      "average loss: 0.6574232578277588\n",
      "average loss: 5.263331582839221e-06\n",
      "average loss: 1.155833664026733e-06\n",
      "average loss: 3.485726106604323e-07\n",
      "average loss: 6.051477563461362e-07\n",
      "average loss: 3.464714061312455e-07\n",
      "average loss: 3.396275038480842e-07\n",
      "average loss: 2.3272407526452277e-07\n",
      " \n",
      "Epoch 2 =======================\n",
      " \n",
      "average loss: 0.03673220053315163\n",
      "average loss: 3.0362991428580263e-06\n",
      "average loss: 5.517982184502502e-07\n",
      "average loss: 1.6803145649524448e-07\n",
      "average loss: 5.641443062262524e-06\n",
      "average loss: 6.252781415862542e-07\n",
      "average loss: 2.3725523689651833e-07\n",
      "average loss: 1.3242998299306668e-07\n",
      " \n",
      "Epoch 3 =======================\n",
      " \n",
      "average loss: 0.048866018652915955\n",
      "average loss: 1.898771823316726e-06\n",
      "average loss: 1.5565290436924211e-06\n",
      "average loss: 1.2170006755576411e-07\n",
      "average loss: 1.762014535518799e-07\n",
      "average loss: 2.105834872965188e-07\n",
      "average loss: 1.5673632276640127e-07\n",
      "average loss: 7.631753511022574e-08\n",
      " \n",
      "Epoch 4 =======================\n",
      " \n",
      "average loss: 0.026623273268342018\n",
      "average loss: 7.026282659400381e-07\n",
      "average loss: 4.867654451906225e-07\n",
      "average loss: 7.129142996575537e-08\n",
      "average loss: 1.525688842033965e-07\n",
      "average loss: 1.4176381346330707e-07\n",
      "average loss: 1.9986550938405405e-07\n",
      "average loss: 5.182829219575177e-08\n",
      " \n",
      "Epoch 5 =======================\n",
      " \n",
      "average loss: 0.02236703410744667\n",
      "average loss: 1.5337704798660569e-06\n",
      "average loss: 2.998113902272192e-07\n",
      "average loss: 1.006358906327198e-07\n",
      "average loss: 1.1993609880084427e-07\n",
      "average loss: 6.639867471467519e-08\n",
      "average loss: 1.4280550591268273e-07\n",
      "average loss: 3.2003890936978295e-08\n"
     ]
    }
   ],
   "source": [
    "# training time!\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\" \")\n",
    "    print(f\"Epoch {epoch+1} =======================\")\n",
    "    print(\" \")\n",
    "    # we will treat each sentence as a batch of inputs\n",
    "    for i, row in enumerate(Xten_train):\n",
    "        running_loss = 0\n",
    "        output = onionnet(row)\n",
    "        loss = criterion(output, y2_train[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i%10000 == 0:\n",
    "            print(f'average loss: {running_loss/(i+1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06eb0153-47d7-46fc-855a-78bd975e174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xten_test = torch.tensor(Xvec_test, dtype=torch.float32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87594c93-bf78-4896-a345-99c9a51aaa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9177144279845637\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for i,row in enumerate(Xten_test):\n",
    "        if onionnet(row).item() >= 0.5:\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = 0\n",
    "        \n",
    "        if prediction == y_test[i]:\n",
    "            correct+=1\n",
    "            \n",
    "    print(correct/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d9db0-d5bd-4772-b640-6dd52fe6c54d",
   "metadata": {},
   "source": [
    "The basic LSTM model improved accuracy over the feed-forward neural network by approximately 1% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8d7a-e781-439e-bffa-0e7c03424b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
