{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75690f6-d3d6-4375-a410-7cc82f16391e",
   "metadata": {},
   "source": [
    "# Classification of Reddit Posts via Natural Language Processing\n",
    "\n",
    "The objective of this project is to correctly classify posts between different subreddits based purely on the text data. For the purposes of this project, we have picked 3 subreddit pairings which the ascending in difficulty based on the content and nature of the posts:\n",
    "1. ```r/datascience``` vs ```r/machinelearning```.\n",
    "2. ```r/news``` vs ```r/theonion```.\n",
    "3. ```r/theonion``` vs ```r/nottheonion```.\n",
    "\n",
    "The idea is to start by prototyping on an easy problem and then progressively ratchet up the (perceived) difficulty to motivate the development of an increasingly complex and robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a30eb0-8639-448e-b496-cad6889feb21",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Data Collection and Scraping\n",
    "\n",
    "We collect posts off reddit using the ```pushshift.io``` API. We package the data-scaping steps and oft-used parameters into helper functions in the custom ```bifrost``` library (this is a reference to the Rainbow Bridge in Norse mythology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464b4944-0294-4d63-b616-34c4bb4cc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import bifrost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c590ef-6504-4a7e-bc90-632da3eb097a",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "Our first step will be to scrape necessary data from the subreddits in question. The first question me must answer is:\n",
    "     \n",
    "<h2 align=\"center\">How much data should we scrape?</h2>\n",
    "     \n",
    "One might think the answer is simple: \"As much as we can get!\" but this is not a satisfactory answer. We could easily scrape 1,000,000 reddit posts from each of the 5 subreddits and this would give us an abundant wealth of data. However, vectorization and training on 2,000,000 text-based examples would be woefully slow and may even be impossible given hardware limitations (imagine trying to store a $2,000,000 \\times 100,000$ matrix in memory; it would probably eat up every last byte of RAM we have and still not be enough!).\n",
    "\n",
    "We must let level-heads prevail and choose a more reasonable number. Let us decide (a bit arbitrarily) on 10,000 posts to train on for each subreddit, with an additional 5,000 posts held-out as a final test set to give our model a final grade. If it turns out this number is too small, we can always go back and scrape more data.\n",
    "\n",
    "We proceed by first scraping the 10,000 training posts from each subreddit and logging the UTC creation times. Then after building our models, we will scrape the additional 5,000 posts with UTC creation times before the last training example (this separation of data scraping is a safe-guard against accidentally training on the test set).\n",
    "\n",
    "We provide the following code as a markdown cell to avoid having to wait for the web-scraping process each time we reboot this notebook, but the reader is free to try running it themselves in a code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66288b31-43b6-430e-8091-6f5a37e8197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_params = {'subreddit':'datascience',\n",
    "             'fields':('title', 'selftext','created_utc'),\n",
    "             'size':100}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58845a-5a5c-4ecb-be45-cc26e1bc64ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "\n",
    "datascience = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( # scrapes subreddit for content, 0.8s time-delay built-in to avoid accidental DDoS\n",
    "                                        content = 'submission',\n",
    "                                        params=ds_params, \n",
    "                                        iters = 100,  # 1 iteration collects 100 posts.\n",
    "                                        verbose = True  # demonstrate process with console printout, will be turned off in future calls\n",
    "                                )\n",
    "                          )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7c6eb-bf95-4ced-94dd-111d16dc0123",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "And just like that, ```bifrost``` has scraped 10,000 reddit posts from ```r/datascience```.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8daaf0d-df26-4545-9b72-566fd15df082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously scraped results \n",
    "# this avoids having to re-scrape each time we reboot the notebook :)\n",
    "datascience = pd.read_csv('../data/datascience10k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001d6aa-814c-46ac-8433-202e0f5bf3e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "# save our data to a csv so we don't have to re-scrape every single time\n",
    "datascience.to_csv('../data/datascience10k.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b641aef9-d351-4ad0-9694-6b48e4fa3b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "datascience.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e206381-2f7f-454e-b5da-45a1461d0f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'created_utc', 'selftext', 'title'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26db551b-19b9-4669-a6e0-97163b539d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "datascience.drop(columns='Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a86f6c-838b-475a-b106-d07a7720c857",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "Let's check for null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a55a18-4910-4e24-bc02-e1003190f6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_utc       0\n",
       "selftext       1726\n",
       "title             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d098b-f175-48cc-b545-28cc577594ff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We have approximately 1700 empty bodies. Based on the author's reddit experience, these are most likely either:\n",
    "1. Questions: the question is in the title so body is empty.\n",
    "2. Content Sharing: the actual content is not text-based, e.g. an embedded youtube video for NLP.\n",
    "3. Removed: the post contents were removed by the moderators.\n",
    "\n",
    "Let's take a quick look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3681a139-06b9-409f-81fb-627a0a31852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1648041965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are your favorite reference books?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1648032739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data scientist returning to work from maternit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1648029230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Logical process order of a sql query and why k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1648024571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hypothesis Testing &amp;amp; Anova Model Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1648018824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Could you give me idea for entry level project...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc selftext                                              title\n",
       "20   1648041965      NaN            What are your favorite reference books?\n",
       "26   1648032739      NaN  Data scientist returning to work from maternit...\n",
       "28   1648029230      NaN  Logical process order of a sql query and why k...\n",
       "30   1648024571      NaN        Hypothesis Testing &amp; Anova Model Topics\n",
       "34   1648018824      NaN  Could you give me idea for entry level project..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience[ datascience['selftext'].isnull() ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97770de-6768-4940-8ac0-f24383f9e13b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's also now scrape the data for the other 4 subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbf38968-d4c0-476f-b078-f3e15bbaf7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_params = {'subreddit':'machinelearning','fields':('title', 'selftext','created_utc'),'size':100}\n",
    "\n",
    "news_params = {'subreddit':'news','fields':('title', 'selftext','created_utc'),'size':100}\n",
    "\n",
    "onion_params = {'subreddit':'theonion','fields':('title', 'selftext','created_utc'),'size':100}\n",
    "\n",
    "notonion_params = {'subreddit':'nottheonion','fields':('title', 'selftext','created_utc'),'size':100}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ce0f1-8c59-45bf-8085-a05854aacacb",
   "metadata": {},
   "source": [
    "```python\n",
    "ml = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit(\n",
    "                                        content = 'submission',\n",
    "                                        params= ml_params, \n",
    "                                        iters = 100,  \n",
    "                                        verbose = False  \n",
    "                                )\n",
    "                          )\n",
    "\n",
    "ml.to_csv('../data/machinelearning10k.csv')\n",
    "```\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54cebc-cd68-408b-b126-167e6807cd2f",
   "metadata": {},
   "source": [
    "```python\n",
    "news = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( \n",
    "                                        content = 'submission',\n",
    "                                        params= news_params, \n",
    "                                        iters = 100,  \n",
    "                                        verbose = False  \n",
    "                                )\n",
    "                          )\n",
    "\n",
    "news.to_csv('../data/news10k.csv')\n",
    "```\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152fa8a-32a3-492f-a2dc-305d55014911",
   "metadata": {},
   "source": [
    "```python\n",
    "onion = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( \n",
    "                                        content = 'submission',\n",
    "                                        params= onion_params, \n",
    "                                        iters = 100, \n",
    "                                        verbose = False  \n",
    "                          )\n",
    "\n",
    "onion.to_csv('../data/onion10k.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9acf2-5f2e-439f-9689-fdaef36ef04e",
   "metadata": {},
   "source": [
    "```python\n",
    "notonion = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( \n",
    "                                        content = 'submission',\n",
    "                                        params= notonion_params, \n",
    "                                        iters = 100, \n",
    "                                        verbose = False  \n",
    "                                )\n",
    "                          )\n",
    "\n",
    "notonion.to_csv('../data/notonion10k.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46868e2d-59ef-40aa-879b-dfb648ef6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = pd.read_csv('../data/machinelearning10k.csv')\n",
    "news = pd.read_csv('../data/news10k.csv')\n",
    "onion = pd.read_csv('../data/onion10k.csv')\n",
    "notonion = pd.read_csv('../data/notonion10k.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54775b-65be-4668-8086-5505679f6f27",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "--- \n",
    "# 100,000 Post Titles\n",
    "\n",
    "For future usage and general data hoarding purposes, we'll scrape the post titles of 100,000 reddits posts in each of the 5 subreddits mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ab7b44-0108-4ba1-9479-87c6b2e5ada2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_epoch = datascience['created_utc'][9999]\n",
    "ml_epoch = ml['created_utc'][9999]\n",
    "news_epoch = news['created_utc'][9999]\n",
    "onion_epoch = onion['created_utc'][9984]\n",
    "notonion_epoch = notonion['created_utc'][9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "403831a3-fd65-43c9-8dbd-a1c9b2cb5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_titles_params =  {'subreddit':'datascience','fields':('title','created_utc'),'size':100, 'before':ds_epoch}\n",
    "\n",
    "ml_titles_params = {'subreddit':'machinelearning','fields':('title','created_utc'),'size':100, 'before':ml_epoch}\n",
    "\n",
    "news_titles_params = {'subreddit':'news','fields':('title','created_utc'),'size':100, 'before':news_epoch}\n",
    "\n",
    "onion_titles_params = {'subreddit':'theonion','fields':('title','created_utc'),'size':100, 'before':onion_epoch}\n",
    "\n",
    "notonion_titles_params = {'subreddit':'nottheonion','fields':('title', 'created_utc'),'size':100, 'before':notonion_epoch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3abb8cf2-5f64-4b1f-957d-45d7e7fa05fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "[Errno Expecting value] <html>\r\n<head><title>429 Too Many Requests</title></head>\r\n<body bgcolor=\"white\">\r\n<center><h1>429 Too Many Requests</h1></center>\r\n<hr><center>nginx/1.14.0</center>\r\n</body>\r\n</html>\r\n: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py:910\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m ds_iter2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m----> 2\u001b[0m                         \u001b[43mbifrost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmass_collect_reddit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubmission\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_titles_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43miters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m                           )\n\u001b[0;32m     11\u001b[0m pd\u001b[38;5;241m.\u001b[39mconcat([datascience[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_utc\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]], ds_iter2])\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/datascience15k.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\desktop\\ghe-dsir222\\projects\\project-3\\code\\bifrost.py:40\u001b[0m, in \u001b[0;36mmass_collect_reddit\u001b[1;34m(content, params, iters, verbose, delay)\u001b[0m\n\u001b[0;32m     38\u001b[0m new_params \u001b[38;5;241m=\u001b[39m params\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[1;32m---> 40\u001b[0m     json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m+\u001b[39m\u001b[43mget_reddit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnew_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     epoch \u001b[38;5;241m=\u001b[39m json[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_utc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     42\u001b[0m     new_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m epoch\n",
      "File \u001b[1;32m~\\desktop\\ghe-dsir222\\projects\\project-3\\code\\bifrost.py:32\u001b[0m, in \u001b[0;36mget_reddit\u001b[1;34m(content, params, verbose)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPassed URL: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(req\u001b[38;5;241m.\u001b[39murl))\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus Code: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(req\u001b[38;5;241m.\u001b[39mstatus_code))\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py:917\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: [Errno Expecting value] <html>\r\n<head><title>429 Too Many Requests</title></head>\r\n<body bgcolor=\"white\">\r\n<center><h1>429 Too Many Requests</h1></center>\r\n<hr><center>nginx/1.14.0</center>\r\n</body>\r\n</html>\r\n: 0"
     ]
    }
   ],
   "source": [
    "ds_iter2 = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( \n",
    "                                        content = 'submission',\n",
    "                                        params=ds_titles_params, \n",
    "                                        iters = 4000,  \n",
    "                                        verbose = True, \n",
    "                                        delay=True\n",
    "                                )\n",
    "                          )\n",
    "\n",
    "pd.concat([datascience[['created_utc','title']], ds_iter2]).to_csv('../data/datascience15k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d512e-f4aa-4fd8-86c6-af7e1c242e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374fae6-5042-4fec-90a1-b61991086b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit( \n",
    "                                        content = 'submission',\n",
    "                                        params= ml_titles_params, \n",
    "                                        iters = 4000,  \n",
    "                                        verbose = False,\n",
    "                                        delay=True\n",
    "                                )\n",
    "                          )\n",
    "\n",
    "ml.to_csv('../data/machinelearningtitles100k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3baf815-3273-4c7e-a85b-a2fb44a64674",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit(\n",
    "                                        content = 'submission',\n",
    "                                        params= news_titles_params, \n",
    "                                        iters = 4000, \n",
    "                                        verbose = False,\n",
    "                                        delay=True\n",
    "                                )\n",
    "                          )\n",
    "\n",
    "news.to_csv('../data/newstitles100k.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100588c-add9-4dfc-9789-38de791c0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onion = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit(\n",
    "                                        content = 'submission',\n",
    "                                        params= onion_titles_params, \n",
    "                                        iters = 4000,  \n",
    "                                        verbose = False,\n",
    "                                        delay=True\n",
    "                                )\n",
    "                          )\n",
    "\n",
    "onion.to_csv('../data/oniontitles100k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac13868-8381-472f-a8a8-33587846fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "notonion = pd.DataFrame(\n",
    "                        bifrost.mass_collect_reddit(\n",
    "                                        content = 'submission',\n",
    "                                        params= notonion_titles_params, \n",
    "                                        iters = 4000,  \n",
    "                                        verbose = False,\n",
    "                                        delay=True\n",
    "                                )\n",
    "                          )\n",
    "\n",
    "notonion.to_csv('../data/notoniontitles100k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef8364-08bd-4822-a471-050771b08e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
